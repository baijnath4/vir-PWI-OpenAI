import { BaseLLM, BaseLLMParams } from 'langchain/llms/base';
import { CallbackManagerForLLMRun } from 'langchain/callbacks';
import { LLMResult, GenerationChunk, MessageType, BaseMessage, ChatResult } from 'langchain/schema';
import { C as Configuration, aT as RequiredPartial, g as GenerateOutput, a6 as PromptTemplateOutput } from '../client-d0435e6d.js';
import { BaseChatModel, BaseChatModelParams } from 'langchain/chat_models/base';
import { PromptTemplate } from 'langchain/prompts';
import 'stream';
import 'node:stream';
import 'zod';
import 'form-data';

interface BaseGenAIModelOptions {
    stream?: boolean;
    parameters?: Record<string, any>;
    timeout?: number;
    configuration?: Configuration;
}
type GenAIModelOptions = (BaseGenAIModelOptions & {
    modelId?: string;
    promptId?: never;
}) | (BaseGenAIModelOptions & {
    modelId?: never;
    promptId: string;
});
declare class GenAIModel extends BaseLLM {
    #private;
    protected modelId?: string;
    protected promptId?: string;
    protected isStreaming: boolean;
    protected timeout: number | undefined;
    protected parameters: Record<string, any>;
    constructor({ modelId, promptId, stream, parameters, timeout, configuration, ...baseParams }: GenAIModelOptions & BaseLLMParams);
    _generate(prompts: string[], options: this['ParsedCallOptions'], runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;
    _streamResponseChunks(_input: string, _options: this['ParsedCallOptions'], _runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;
    getNumTokens(input: string): Promise<number>;
    _modelType(): string;
    _llmType(): string;
}

interface GenAILLMOutput {
    tokenUsage: Pick<GenerateOutput, 'input_token_count' | 'generated_token_count'>;
}
type RolesMapping = RequiredPartial<Record<MessageType, {
    stopSequence: string;
}>, 'system'>;
type Options = BaseChatModelParams & GenAIModelOptions & {
    rolesMapping: RolesMapping;
};
declare class GenAIChatModel extends BaseChatModel {
    #private;
    constructor(options: Options);
    _generate(messages: BaseMessage[], options: this['ParsedCallOptions'], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _combineLLMOutput(...llmOutputs: GenerateOutput[]): GenAILLMOutput;
    _llmType(): string;
    _modelType(): string;
}

declare class GenAIPromptTemplate {
    static toLangChain(template: PromptTemplateOutput | PromptTemplateOutput['value']): PromptTemplate;
    static fromLangChain(template: PromptTemplate): PromptTemplateOutput['value'];
    private static getTemplateMatcher;
}

export { GenAIChatModel, GenAIModel, GenAIModelOptions, GenAIPromptTemplate, RolesMapping };
