import {
  Client,
  InvalidInputError,
  asyncGeneratorToArray,
  concatUnique,
  isNotEmptyArray,
  isNullish
} from "../chunk-LHGFTNJV.js";

// src/langchain/llm.ts
import { BaseLLM } from "langchain/llms/base";
import { GenerationChunk } from "langchain/schema";
var GenAIModel = class extends BaseLLM {
  #client;
  modelId;
  promptId;
  isStreaming;
  timeout;
  parameters;
  constructor({
    modelId,
    promptId,
    stream = false,
    parameters,
    timeout,
    configuration,
    ...baseParams
  }) {
    super(baseParams ?? {});
    this.modelId = modelId;
    this.promptId = promptId;
    this.timeout = timeout;
    this.isStreaming = Boolean(stream);
    this.parameters = parameters || {};
    this.#client = new Client(configuration);
  }
  #createPayload(prompts, options) {
    const stopSequences = concatUnique(this.parameters.stop, options.stop);
    return prompts.map((input) => ({
      ...!isNullish(this.promptId) ? {
        prompt_id: this.promptId
      } : !isNullish(this.modelId) ? {
        model_id: this.modelId
      } : {},
      input,
      parameters: {
        ...this.parameters,
        stop_sequences: isNotEmptyArray(stopSequences) ? stopSequences : void 0
      }
    }));
  }
  async #execute(prompts, options) {
    return await Promise.all(
      this.#client.generate(this.#createPayload(prompts, options), {
        signal: options.signal,
        timeout: this.timeout,
        stream: false
      })
    );
  }
  async _generate(prompts, options, runManager) {
    const response = [];
    if (this.isStreaming) {
      const { output } = await asyncGeneratorToArray(
        this._streamResponseChunks(prompts[0], options, runManager)
      );
      response.push(output);
    } else {
      const outputs = await this.#execute(prompts, options);
      response.push(...outputs);
    }
    const generations = response.map(
      ({ generated_text: text, ...generationInfo }) => [
        {
          text,
          generationInfo
        }
      ]
    );
    const llmOutput = await response.reduce(
      (acc, generation) => {
        acc.generated_token_count += generation.generated_token_count;
        acc.input_token_count += generation.input_token_count;
        return acc;
      },
      {
        generated_token_count: 0,
        input_token_count: 0
      }
    );
    return { generations, llmOutput };
  }
  async *_streamResponseChunks(_input, _options, _runManager) {
    const [payload] = this.#createPayload([_input], _options);
    const stream = this.#client.generate(payload, {
      signal: _options.signal,
      timeout: this.timeout,
      stream: true
    });
    const fullOutput = {
      generated_text: "",
      stop_reason: "NOT_FINISHED",
      input_token_count: 0,
      generated_token_count: 0
    };
    for await (const { generated_text, ...chunk } of stream) {
      const generation = new GenerationChunk({
        text: generated_text,
        generationInfo: chunk
      });
      yield generation;
      void _runManager?.handleLLMNewToken(generated_text);
      fullOutput.generated_text += generation.text;
      if (chunk.stop_reason) {
        fullOutput.stop_reason = chunk.stop_reason;
      }
      fullOutput.input_token_count += chunk.input_token_count;
      fullOutput.generated_token_count += chunk.generated_token_count;
    }
    return fullOutput;
  }
  async getNumTokens(input) {
    const result = await this.#client.tokenize({
      ...!isNullish(this.modelId) && {
        model_id: this.modelId
      },
      input,
      parameters: {
        return_tokens: false
      }
    });
    return result.token_count ?? 0;
  }
  _modelType() {
    return this.modelId ?? "default";
  }
  _llmType() {
    return "GenAI";
  }
};

// src/langchain/llm-chat.ts
import { BaseChatModel } from "langchain/chat_models/base";
import {
  SystemMessage
} from "langchain/schema";
var GenAIChatModel = class extends BaseChatModel {
  #model;
  #rolesMapping;
  constructor(options) {
    super(options);
    this.#rolesMapping = options.rolesMapping;
    this.#model = new GenAIModel({
      ...options,
      parameters: {
        ...options.parameters,
        stop_sequences: concatUnique(
          options.parameters?.stop_sequences,
          Object.values(options.rolesMapping).map((role) => role.stopSequence)
        )
      },
      configuration: {
        ...options.configuration,
        retries: options.maxRetries ?? options.configuration?.retries
      }
    });
  }
  async _generate(messages, options, runManager) {
    const message = messages.map((msg) => {
      const type = this.#rolesMapping[msg._getType()];
      if (!type) {
        throw new InvalidInputError(
          `Unsupported message type "${msg._getType()}"`
        );
      }
      return `${type.stopSequence}${msg.text}`;
    }).join("\n").concat(this.#rolesMapping.system.stopSequence);
    const output = await this.#model._generate([message], options, runManager);
    return {
      generations: output.generations.map(([generation]) => ({
        message: new SystemMessage(generation.text),
        generationInfo: generation.generationInfo,
        text: generation.text
      })),
      llmOutput: output.llmOutput
    };
  }
  _combineLLMOutput(...llmOutputs) {
    return llmOutputs.reduce(
      (acc, gen) => {
        acc.tokenUsage.generated_token_count += gen.generated_token_count || 0;
        acc.tokenUsage.input_token_count += gen.input_token_count || 0;
        return acc;
      },
      {
        tokenUsage: {
          generated_token_count: 0,
          input_token_count: 0
        }
      }
    );
  }
  _llmType() {
    return "GenAIChat";
  }
  _modelType() {
    return this.#model._modelType();
  }
};

// src/langchain/prompt-template.ts
import { PromptTemplate as LangChainPromptTemplate } from "langchain/prompts";
var GenAIPromptTemplate = class _GenAIPromptTemplate {
  static toLangChain(template) {
    const body = typeof template === "string" ? template : template.value;
    const fString = body.replace(
      _GenAIPromptTemplate.getTemplateMatcher("mustache"),
      "{$1}"
    );
    return LangChainPromptTemplate.fromTemplate(fString, {
      templateFormat: "f-string",
      validateTemplate: true
    });
  }
  static fromLangChain(template) {
    return template.template.replace(
      _GenAIPromptTemplate.getTemplateMatcher(template.templateFormat),
      "{{$1}}"
    );
  }
  static getTemplateMatcher(name) {
    switch (name) {
      case "mustache":
        return /\{\{([^}]+)\}\}/g;
      case "jinja2":
        return /\{\{\s*(.*?)\s*\}\}/g;
      case "fstring":
      case "f-string":
        return /\{([^}]+)\}/g;
      default: {
        throw new InvalidInputError(`Unknown template format "${name}".`);
      }
    }
  }
};
export {
  GenAIChatModel,
  GenAIModel,
  GenAIPromptTemplate
};
