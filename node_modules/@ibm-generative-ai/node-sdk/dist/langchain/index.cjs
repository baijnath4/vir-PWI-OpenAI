"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/langchain/index.ts
var langchain_exports = {};
__export(langchain_exports, {
  GenAIChatModel: () => GenAIChatModel,
  GenAIModel: () => GenAIModel,
  GenAIPromptTemplate: () => GenAIPromptTemplate
});
module.exports = __toCommonJS(langchain_exports);

// src/langchain/llm.ts
var import_base = require("langchain/llms/base");
var import_schema = require("langchain/schema");

// src/client/client.ts
var import_node_http = __toESM(require("http"), 1);
var import_node_https = __toESM(require("https"), 1);
var import_node_stream = require("stream");
var import_axios2 = __toESM(require("axios"), 1);
var import_form_data = __toESM(require("form-data"), 1);
var import_axios_cache_interceptor = require("axios-cache-interceptor");
var import_promise_retry = __toESM(require("promise-retry"), 1);
var import_node_fetch_event_source = require("@ai-zen/node-fetch-event-source");

// src/api-types.ts
var import_zod = require("zod");
var PaginationOutputSchema = import_zod.z.object({
  totalCount: import_zod.z.number().int().nonnegative(),
  results: import_zod.z.array(import_zod.z.any())
});
var UserGenerateDefaultInputSchema = import_zod.z.object({
  model_id: import_zod.z.string(),
  parameters: import_zod.z.optional(import_zod.z.record(import_zod.z.any()))
});
var ParametersSchema = import_zod.z.record(import_zod.z.any());
var GenerateInputSchema = import_zod.z.object({
  model_id: import_zod.z.string().nullish(),
  prompt_id: import_zod.z.string().nullish(),
  inputs: import_zod.z.array(import_zod.z.string()),
  parameters: import_zod.z.optional(ParametersSchema),
  use_default: import_zod.z.optional(import_zod.z.boolean())
});
var GenerateStopReasonSchema = import_zod.z.enum([
  "NOT_FINISHED",
  "MAX_TOKENS",
  "EOS_TOKEN",
  "CANCELLED",
  "TIME_LIMIT",
  "STOP_SEQUENCE",
  "TOKEN_LIMIT",
  "ERROR"
]);
var GenerateModerationSchema = import_zod.z.object({
  hap: import_zod.z.optional(
    import_zod.z.array(
      import_zod.z.object({
        success: import_zod.z.boolean(),
        flagged: import_zod.z.boolean(),
        score: import_zod.z.number().min(0).max(1),
        position: import_zod.z.object({
          start: import_zod.z.number().int().min(0),
          stop: import_zod.z.number().int().min(0)
        })
      }).passthrough()
    )
  )
}).passthrough();
var GenerateResultSchema = import_zod.z.object({
  generated_text: import_zod.z.string(),
  generated_token_count: import_zod.z.number().int().min(0),
  input_token_count: import_zod.z.number().int().min(0),
  stop_reason: GenerateStopReasonSchema
}).passthrough();
var GenerateOutputSchema = import_zod.z.object({
  model_id: import_zod.z.string(),
  created_at: import_zod.z.coerce.date(),
  results: import_zod.z.array(GenerateResultSchema),
  moderation: GenerateModerationSchema.optional()
}).passthrough();
var GenerateLimitsOutputSchema = import_zod.z.object({
  tokenCapacity: import_zod.z.number().int().nonnegative(),
  tokensUsed: import_zod.z.number().int().nonnegative()
});
var GenerateConfigInputSchema = import_zod.z.object({
  model_id: import_zod.z.optional(import_zod.z.string()),
  parameters: import_zod.z.optional(import_zod.z.record(import_zod.z.any()))
});
var GenerateConfigOutputSchema = import_zod.z.object({
  model_id: import_zod.z.string().nullish(),
  parameters: import_zod.z.record(import_zod.z.any()).nullish()
});
var TokenizeInputSchema = import_zod.z.object({
  model_id: import_zod.z.string().nullish(),
  inputs: import_zod.z.array(import_zod.z.string()),
  use_default: import_zod.z.optional(import_zod.z.boolean()),
  parameters: import_zod.z.optional(import_zod.z.object({ return_tokens: import_zod.z.optional(import_zod.z.boolean()) }))
});
var TokenizeOutputSchema = import_zod.z.object({
  model_id: import_zod.z.string(),
  created_at: import_zod.z.string(),
  results: import_zod.z.array(
    import_zod.z.object({
      token_count: import_zod.z.number().int().nonnegative(),
      tokens: import_zod.z.array(import_zod.z.string())
    })
  )
});
var ModelsOutputSchema = import_zod.z.object({
  results: import_zod.z.array(
    import_zod.z.object({
      id: import_zod.z.string(),
      name: import_zod.z.string(),
      size: import_zod.z.string(),
      token_limit: import_zod.z.number().int().nonnegative()
    })
  )
});
var ModelSchemaSchema = import_zod.z.object({ id: import_zod.z.number().int(), value: import_zod.z.any() });
var ModelOutputSchema = import_zod.z.object({
  results: import_zod.z.object({
    id: import_zod.z.string(),
    name: import_zod.z.string(),
    size: import_zod.z.string(),
    description: import_zod.z.string(),
    token_limit: import_zod.z.number().int().nonnegative(),
    tags: import_zod.z.array(import_zod.z.string()),
    source_model_id: import_zod.z.string().nullable(),
    tasks: import_zod.z.array(
      import_zod.z.object({
        id: import_zod.z.string(),
        name: import_zod.z.string(),
        json_example: import_zod.z.string(),
        jsonl_example: import_zod.z.string()
      })
    ),
    model_family: import_zod.z.object({
      id: import_zod.z.number().int(),
      name: import_zod.z.string(),
      short_description: import_zod.z.string().nullish(),
      description: import_zod.z.string().nullish()
    }),
    schema_generate: ModelSchemaSchema,
    schema_tokenize: ModelSchemaSchema
  })
});
var TuneStatusSchema = import_zod.z.enum([
  "INITIALIZING",
  "NOT_STARTED",
  "PENDING",
  "HALTED",
  "RUNNING",
  "QUEUED",
  "COMPLETED",
  "FAILED"
]);
var TuneFileSchema = import_zod.z.object({
  id: import_zod.z.string(),
  file_name: import_zod.z.string(),
  created_at: import_zod.z.string()
});
var TuneMixinSchema = import_zod.z.object({
  id: import_zod.z.string(),
  name: import_zod.z.string(),
  model_id: import_zod.z.string(),
  method_id: import_zod.z.string(),
  model_name: import_zod.z.string(),
  status: TuneStatusSchema,
  task_id: import_zod.z.string(),
  parameters: import_zod.z.object({
    batch_size: import_zod.z.number().int().positive(),
    num_epochs: import_zod.z.number().int().positive()
  }),
  created_at: import_zod.z.string()
});
var TuneInputSchema = import_zod.z.object({
  name: import_zod.z.string(),
  model_id: import_zod.z.string(),
  task_id: import_zod.z.string(),
  training_file_ids: import_zod.z.array(import_zod.z.string()),
  validation_file_ids: import_zod.z.array(import_zod.z.string()).nullish(),
  evaluation_file_ids: import_zod.z.array(import_zod.z.string()).nullish(),
  method_id: import_zod.z.string(),
  parameters: import_zod.z.record(import_zod.z.any()).nullish()
});
var TuneOutputSchema = import_zod.z.object({
  results: TuneMixinSchema.extend({
    validation_files: import_zod.z.array(TuneFileSchema).nullish(),
    training_files: import_zod.z.array(TuneFileSchema).nullish(),
    evaluation_files: import_zod.z.array(TuneFileSchema).nullish(),
    datapoints: import_zod.z.object({
      loss: import_zod.z.array(
        import_zod.z.object({
          data: import_zod.z.any(),
          timestamp: import_zod.z.string()
        })
      )
    }).nullish()
  })
});
var TuneMethodsOutputSchema = import_zod.z.object({
  results: import_zod.z.array(
    import_zod.z.object({
      id: import_zod.z.string(),
      name: import_zod.z.string()
    })
  )
});
var PromptTemplateInputSchema = import_zod.z.object({
  id: import_zod.z.string()
}).strict();
var PromptTemplateCreateInputSchema = import_zod.z.object({
  name: import_zod.z.string(),
  value: import_zod.z.string()
}).strict();
var SinglePromptTemplateOutputSchema = import_zod.z.object({
  id: import_zod.z.string(),
  name: import_zod.z.string(),
  value: import_zod.z.string(),
  created_at: import_zod.z.coerce.date()
}).passthrough();
var PromptTemplateOutputSchema = import_zod.z.object({
  results: SinglePromptTemplateOutputSchema
});
var PromptTemplatesOutputSchema = PaginationOutputSchema.extend({
  results: import_zod.z.array(SinglePromptTemplateOutputSchema)
}).passthrough();
var PromptTemplateExecuteInputSchema = import_zod.z.object({
  inputs: import_zod.z.array(import_zod.z.string()),
  template: import_zod.z.union([
    import_zod.z.object({ id: import_zod.z.string() }),
    import_zod.z.object({
      value: import_zod.z.string(),
      data: import_zod.z.object({}).passthrough()
    })
  ])
});
var PromptTemplateExecuteOutputSchema = import_zod.z.object({
  results: import_zod.z.array(import_zod.z.string())
});
var HistoryStatusSchema = import_zod.z.enum(["SUCCESS", "ERROR"]);
var HistoryOriginSchema = import_zod.z.enum(["API", "UI"]);
var HistoryInputSchema = import_zod.z.object({
  status: HistoryStatusSchema,
  origin: HistoryOriginSchema
}).partial();
var HistoryOutputSchema = PaginationOutputSchema.extend({
  results: import_zod.z.array(
    import_zod.z.object({
      id: import_zod.z.string(),
      duration: import_zod.z.number().int().min(0),
      request: GenerateInputSchema.partial(),
      status: HistoryInputSchema.shape.status,
      created_at: import_zod.z.coerce.date(),
      response: GenerateOutputSchema.nullable()
    }).passthrough()
  )
});
var FilePurposeSchema = import_zod.z.enum(["tune", "template", "tune_import"]);
var FileInputSchema = import_zod.z.object({
  id: import_zod.z.string()
}).strict();
var FileCreateInputSchema = import_zod.z.custom();
var SingleFileOutputSchema = import_zod.z.object({
  id: import_zod.z.string(),
  file_name: import_zod.z.string(),
  purpose: FilePurposeSchema,
  created_at: import_zod.z.coerce.date()
}).passthrough();
var FileOutputSchema = import_zod.z.object({
  results: SingleFileOutputSchema
});
var FilesOutputSchema = PaginationOutputSchema.extend({
  results: import_zod.z.array(SingleFileOutputSchema)
});
var ChatRoleSchema = import_zod.z.enum(["user", "system", "assistant"]);
var ChatInputSchema = import_zod.z.object({
  model_id: import_zod.z.string(),
  messages: import_zod.z.array(
    import_zod.z.object({
      role: ChatRoleSchema,
      content: import_zod.z.string()
    })
  ),
  conversation_id: import_zod.z.string().nullish(),
  parent_id: import_zod.z.string().nullish(),
  prompt_id: import_zod.z.string().nullish(),
  parameters: ParametersSchema.nullish()
});
var ChatOutputSchema = import_zod.z.object({
  conversation_id: import_zod.z.string(),
  results: import_zod.z.array(
    import_zod.z.object({
      generated_text: import_zod.z.string()
    }).partial()
  )
});

// src/errors.ts
var import_axios = require("axios");
var BaseError = class extends Error {
};
var InvalidInputError = class extends BaseError {
};
var InternalError = class extends BaseError {
};
var RequestError = class extends BaseError {
  code;
  cancelled;
  constructor(message, code, cancelled = false, options) {
    super(message, options);
    this.name = new.target.name;
    this.code = code;
    this.cancelled = cancelled;
    Object.setPrototypeOf(this, new.target.prototype);
    Error.captureStackTrace(this, this.constructor);
  }
};
var RequestCanceledError = class extends RequestError {
  constructor(message, code, options) {
    super(message, code, true, options);
  }
};
var HttpError = class extends RequestError {
  statusCode;
  extensions;
  response;
  headers;
  constructor(message, statusText, statusCode, extensions, response, headers, options) {
    super(message, "ERR_NON_2XX_3XX_RESPONSE", false, options);
    this.statusCode = statusCode;
    this.extensions = extensions;
    this.response = response;
    this.headers = headers;
  }
};
function isAbortError(err) {
  return Boolean(err && err instanceof Error && err.name === "AbortError");
}
function errorTransformer(err) {
  if (isAbortError(err)) {
    return new RequestCanceledError(
      err.message,
      String(err.cause ?? err.name),
      {
        cause: err
      }
    );
  }
  if (!(0, import_axios.isAxiosError)(err)) {
    return err;
  }
  if (err.response) {
    const response = err.response;
    return new HttpError(
      response.data.message,
      response.statusText,
      response.status,
      response.data.extensions,
      response.data,
      response.headers instanceof import_axios.AxiosHeaders ? response.headers.toJSON() : (
        // There's some inconsistency between node headers and axios headers types (and only types because we are not in the browser)
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        response.headers
      ),
      { cause: err }
    );
  }
  if ((0, import_axios.isCancel)(err)) {
    return new RequestCanceledError(err.message, err.code, { cause: err });
  }
  return new RequestError(err.message, err.code, false, { cause: err });
}
function isRetrievableError(error) {
  if (error instanceof HttpError) {
    return [
      // Client errors
      import_axios.HttpStatusCode.RequestTimeout,
      import_axios.HttpStatusCode.TooManyRequests,
      // Server errors
      import_axios.HttpStatusCode.InternalServerError,
      import_axios.HttpStatusCode.BadGateway,
      import_axios.HttpStatusCode.ServiceUnavailable,
      import_axios.HttpStatusCode.GatewayTimeout,
      import_axios.HttpStatusCode.InsufficientStorage
    ].includes(error.statusCode);
  }
  if (error instanceof RequestError) {
    return !error.cancelled;
  }
  return false;
}

// src/buildInfo.ts
var version = "1.5.0";

// src/helpers/common.ts
var import_node_util = require("util");
var import_node_url = require("url");
var import_zod2 = require("zod");
function isTruthy(value) {
  return Boolean(value);
}
function concatUnique(...arrays) {
  const merged = arrays.filter(isTruthy).flat();
  return Array.from(new Set(merged).values());
}
function isNotEmptyArray(arr) {
  return Array.isArray(arr) && arr.length > 0;
}
async function wait(ms) {
  await new Promise((resolve) => setTimeout(resolve, ms));
}
function isFunction(value) {
  return import_zod2.z.function().safeParse(value).success;
}
function safeParseJson(value) {
  try {
    return JSON.parse(typeof value === "string" ? value : String(value));
  } catch {
    return null;
  }
}
function parseFunctionOverloads(inputOrOptionsOrCallback, optionsOrCallback, callback) {
  if (isFunction(inputOrOptionsOrCallback)) {
    return { callback: inputOrOptionsOrCallback };
  }
  const input = inputOrOptionsOrCallback;
  const options = isFunction(optionsOrCallback) ? void 0 : optionsOrCallback;
  const cb = isFunction(optionsOrCallback) ? optionsOrCallback : callback;
  return {
    input,
    options,
    callback: cb
  };
}
function handle(params, executor) {
  const { input, options, callback } = parseFunctionOverloads(
    params.inputOrOptionsOrCallback,
    params.optionsOrCallback,
    params.callback
  );
  const executorWrapper = () => executor({
    input,
    options
  });
  if (callback) {
    return (0, import_node_util.callbackify)(executorWrapper)(callback);
  }
  return executorWrapper();
}
function handleGenerator(params, executor) {
  const { input, options, callback } = parseFunctionOverloads(
    params.inputOrOptionsOrCallback,
    params.optionsOrCallback,
    params.callback
  );
  const executorWrapper = () => executor({
    input,
    options
  });
  if (callback) {
    return callbackifyGenerator(executorWrapper)(callback);
  }
  return executorWrapper();
}
function isTypeOf(value, result) {
  return result;
}
function isNullish(value) {
  return value === null || value === void 0;
}
function callbackifyGenerator(generatorFn) {
  return (callback) => {
    (async () => {
      try {
        for await (const result of generatorFn()) {
          callback(null, result);
        }
      } catch (err) {
        callback(err);
      }
    })();
  };
}
function callbackifyStream(stream) {
  return (callbackFn) => {
    stream.on("data", (data) => callbackFn(null, data));
    stream.on("error", (err) => callbackFn(err));
    stream.on(
      "finish",
      () => callbackFn(null, null)
    );
  };
}
function callbackifyPromise(promise) {
  return (callbackFn) => {
    promise.then(
      (data) => callbackFn(null, data),
      (err) => callbackFn(err)
    );
  };
}
async function* paginator(executor, {
  offset = 0,
  count = Infinity,
  params,
  limit = 100
}) {
  let currentOffset = offset;
  let remainingCount = count;
  let totalCount = Infinity;
  while (currentOffset < totalCount) {
    const paginatedSearchParams = new import_node_url.URLSearchParams(params);
    paginatedSearchParams.set("offset", currentOffset.toString());
    paginatedSearchParams.set(
      "limit",
      Math.min(remainingCount, limit).toString()
    );
    const output = await executor(paginatedSearchParams);
    for (const result of output.results) {
      yield result;
      if (--remainingCount === 0)
        return;
      ++currentOffset;
    }
    totalCount = output.totalCount;
  }
}
function isEmptyObject(obj) {
  for (const key in obj) {
    if (Object.prototype.hasOwnProperty.call(obj, key)) {
      return false;
    }
  }
  return true;
}
async function asyncGeneratorToArray(generator) {
  const response = {
    chunks: [],
    output: void 0
  };
  while (true) {
    const { done, value } = await generator.next();
    if (done) {
      response.output = value;
      break;
    }
    response.chunks.push(value);
  }
  return response;
}

// src/utils/stream.ts
var import_stream = require("stream");
var TypedReadable = class extends import_stream.Readable {
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  _read(size) {
  }
  addListener(event, listener) {
    return super.addListener(event, listener);
  }
  on(event, listener) {
    return super.on(event, listener);
  }
  [Symbol.asyncIterator]() {
    return super[Symbol.asyncIterator]();
  }
};

// src/helpers/config.ts
var import_node_os = __toESM(require("os"), 1);
var import_node_path = __toESM(require("path"), 1);
var import_node_fs = __toESM(require("fs"), 1);
var import_yaml = __toESM(require("yaml"), 1);
function lookupEndpoint() {
  return process.env.GENAI_ENDPOINT || "https://workbench-api.res.ibm.com";
}
function lookupApiKey() {
  if (process.env.GENAI_API_KEY) {
    return process.env.GENAI_API_KEY;
  }
  const credentialsPath = import_node_path.default.join(import_node_os.default.homedir(), ".genai", "credentials.yml");
  if (import_node_fs.default.existsSync(credentialsPath)) {
    try {
      const fileContent = import_node_fs.default.readFileSync(credentialsPath, "utf8");
      return import_yaml.default.parse(fileContent).apiKey;
    } catch (err) {
      console.warn("Failed to read credentials");
    }
  }
  return null;
}

// src/constants.ts
var RETRY_ATTEMPTS_DEFAULT = 3;

// src/client/cache.ts
var CacheDiscriminator = {
  GENERATE_CONFIG: "generate-config",
  TUNE: "tune",
  PROMPT_TEMPLATE: "prompt-template",
  FILE: "file",
  MODEL: "model",
  MODELS: "models"
};
function generateCacheKey(discriminator, id = "") {
  return `${discriminator}#${id}`;
}

// src/client/client.ts
var Client = class {
  #client;
  #options;
  constructor(config = {}) {
    const endpoint = config.endpoint ?? lookupEndpoint();
    if (!endpoint) {
      throw new InvalidInputError("Configuration endpoint is missing!");
    }
    const apiKey = config.apiKey ?? lookupApiKey();
    if (!apiKey) {
      throw new InvalidInputError("Configuration API key is missing!");
    }
    const agent = version ? `node-sdk/${version}` : "node-sdk";
    this.#options = {
      endpoint,
      apiKey,
      headers: {
        "User-Agent": agent,
        "X-Request-Origin": agent,
        ...config.headers,
        Accept: "application/json",
        Authorization: `Bearer ${apiKey}`
      },
      retries: config.retries ?? RETRY_ATTEMPTS_DEFAULT
    };
    this.#client = (0, import_axios_cache_interceptor.setupCache)(
      import_axios2.default.create({
        baseURL: this.#options.endpoint,
        headers: this.#options.headers,
        httpAgent: new import_node_http.default.Agent({ keepAlive: true }),
        httpsAgent: new import_node_https.default.Agent({ keepAlive: true }),
        maxRedirects: 0,
        transitional: {
          clarifyTimeoutError: true
        }
      })
    );
  }
  #fetcher(input, schema) {
    if (input.stream) {
      const outputStream = new TypedReadable({
        autoDestroy: true,
        objectMode: true,
        signal: input.signal
      });
      const onClose = () => {
        if (outputStream.readable) {
          outputStream.push(null);
        }
      };
      const delegatedController = new AbortController();
      if (input.signal) {
        input.signal.addEventListener(
          "abort",
          () => {
            delegatedController.abort();
          },
          {
            once: true
          }
        );
      }
      const onError = (e) => {
        const err = errorTransformer(e);
        delegatedController.abort();
        if (outputStream.readable) {
          outputStream.emit("error", err);
          throw err;
        }
        onClose();
      };
      const url = new URL(
        input.url ?? this.#options.endpoint,
        this.#options.endpoint
      );
      (0, import_node_fetch_event_source.fetchEventSource)(url.toString(), {
        method: "POST",
        body: JSON.stringify(input.data),
        headers: {
          ...this.#options.headers,
          "Content-Type": "application/json"
        },
        signal: delegatedController.signal,
        onclose: onClose,
        async onopen(response) {
          const contentType = response.headers.get("content-type") || "";
          if (response.ok && contentType === import_node_fetch_event_source.EventStreamContentType) {
            return;
          }
          const responseData = contentType.startsWith("application/json") ? await response.json().catch(() => null) : null;
          const headers = (() => {
            const obj = {};
            response.headers?.forEach((value, key) => {
              obj[key] = value;
            });
            return obj;
          })();
          onError(
            new HttpError(
              responseData?.message || "Invalid response from server",
              response.statusText,
              response.status,
              responseData?.extensions,
              responseData,
              headers
            )
          );
        },
        onmessage(message) {
          if (message.event === "close") {
            onClose();
            return;
          }
          if (message.data === "") {
            return;
          }
          const result = safeParseJson(message.data);
          if (result === null) {
            onError(
              new InternalError(
                `Failed to parse message "${JSON.stringify(message)}"`
              )
            );
            return;
          }
          outputStream.push(schema ? schema.parse(result) : result);
        },
        onerror: onError
      }).catch(() => {
      });
      return outputStream;
    }
    const { retries, retryCondition, cache, ...restConfig } = input;
    return (0, import_promise_retry.default)(
      (retry, attempt) => this.#client({
        ...restConfig,
        timeout: input.timeout === void 0 || input.timeout === Infinity ? 0 : Math.max(1, input.timeout),
        cache: {
          ...cache,
          override: (cache ? cache.override : false) || attempt > 1
        }
      }).catch((err) => {
        const error = errorTransformer(err);
        const conditionFn = retryCondition ?? isRetrievableError;
        if (conditionFn(error)) {
          retry(error);
        }
        throw error;
      }),
      { retries: retries ?? this.#options.retries }
    ).then(({ data }) => schema ? schema.parse(data) : data);
  }
  tokenize({ input, ...restInput }, optionsOrCallback, callback) {
    return handle(
      {
        optionsOrCallback,
        callback
      },
      async ({ options }) => {
        const { results } = await this.#fetcher({
          ...options,
          method: "POST",
          url: "/v1/tokenize",
          data: {
            ...restInput,
            use_default: true,
            inputs: [input]
          },
          stream: false
        });
        if (results.length !== 1) {
          throw new InvalidInputError("Unexpected number of results");
        }
        return results[0];
      }
    );
  }
  generate(input, optionsOrCallback, callbackOrNothing) {
    const { callback, options } = parseFunctionOverloads(
      void 0,
      optionsOrCallback,
      callbackOrNothing
    );
    const getTimeout = (() => {
      const start = Date.now();
      const timeout = options?.timeout ?? this.#client.defaults.timeout;
      return () => Math.max(0, timeout ? timeout - (Date.now() - start) : Infinity);
    })();
    const inputs = !Array.isArray(input) ? [input] : input;
    const prepareRequest = ({
      input: inputText,
      ...params
    }) => ({
      ...options,
      method: "POST",
      url: "/v1/generate",
      data: {
        ...params,
        inputs: [inputText],
        use_default: !params.prompt_id,
        parameters: {
          ...params.parameters,
          stream: Boolean(options?.stream)
        }
      }
    });
    if (options?.stream) {
      if (inputs.length > 1) {
        throw new InvalidInputError(
          "Cannot do streaming for more than one input!"
        );
      }
      const stream = new import_node_stream.Transform({
        autoDestroy: true,
        objectMode: true,
        transform(chunk, encoding, callback2) {
          try {
            const {
              generated_text = "",
              stop_reason = null,
              input_token_count = 0,
              generated_token_count = 0,
              ...props
            } = (chunk.results || [{}])[0];
            callback2(null, {
              generated_text,
              stop_reason,
              input_token_count,
              generated_token_count,
              ...chunk.moderation && {
                moderation: chunk.moderation
              },
              ...props
            });
          } catch (e) {
            const err = chunk || e;
            callback2(err, null);
          }
        }
      });
      this.#fetcher({
        ...prepareRequest(inputs[0]),
        timeout: getTimeout(),
        stream: true
      }).on("error", (err) => stream.emit("error", errorTransformer(err))).pipe(stream);
      if (!callback) {
        return stream;
      }
      callbackifyStream(stream)(callback);
      return;
    }
    const tokenCounts = inputs.map(() => 1);
    const promises = inputs.map(async (inputData, index, arr) => {
      try {
        while (getTimeout() > 0) {
          const limits = await this.generateLimits(void 0, {
            ...options,
            timeout: getTimeout()
          });
          const cumulativeTokenCount = tokenCounts.slice(0, index + 1).reduce((acc, value) => acc + value, 0);
          const isWithinLimits = limits.tokensUsed + cumulativeTokenCount <= limits.tokenCapacity;
          if (isWithinLimits) {
            try {
              const { results } = await this.#fetcher({
                ...prepareRequest(inputData),
                timeout: getTimeout()
              });
              if (results.length !== 1) {
                throw new InternalError("Unexpected number of results");
              }
              return results[0];
            } catch (err) {
              if (err instanceof HttpError && err.extensions?.code === "TOO_MANY_REQUESTS" && err.extensions?.reason === "CONCURRENCY_LIMIT") {
                continue;
              }
              throw err;
            }
          }
          await wait(Math.min(getTimeout(), 1e3));
        }
        throw new import_axios2.AxiosError("Timeout exceeded", import_axios2.AxiosError.ETIMEDOUT);
      } finally {
        tokenCounts[index] = 0;
        await Promise.allSettled(arr.slice(0, index));
      }
    });
    if (callback) {
      promises.forEach((promise) => callbackifyPromise(promise)(callback));
    } else {
      return Array.isArray(input) ? promises : promises[0];
    }
  }
  generateConfig(inputOrResetOptions, optionsOrCallback, callback) {
    return handle(
      {
        inputOrOptionsOrCallback: inputOrResetOptions,
        optionsOrCallback,
        callback
      },
      ({ input, options }) => {
        const cacheKey = generateCacheKey(CacheDiscriminator.GENERATE_CONFIG);
        if (isTypeOf(
          input,
          !input || "reset" in input
        )) {
          const { reset, ...httpOptions2 } = input ?? {};
          if (reset) {
            return this.#fetcher({
              ...httpOptions2,
              method: "DELETE",
              url: "/v1/generate/config",
              cache: {
                update: {
                  [cacheKey]: "delete"
                }
              }
            });
          } else {
            return this.#fetcher({
              ...httpOptions2,
              method: "GET",
              url: "/v1/generate/config",
              id: cacheKey
            });
          }
        }
        const { strategy, ...httpOptions } = options ?? {};
        return this.#fetcher({
          ...httpOptions,
          method: strategy === "merge" ? "PATCH" : "PUT",
          url: "/v1/generate/config",
          stream: false,
          data: input,
          cache: {
            update: {
              [cacheKey]: "delete"
            }
          }
        });
      }
    );
  }
  generateLimits(inputOrCallback, optionsOrCallback, callback) {
    return handle(
      {
        inputOrOptionsOrCallback: inputOrCallback,
        optionsOrCallback,
        callback
      },
      ({ options }) => this.#fetcher({
        ...options,
        method: "GET",
        url: "/v1/generate/limits",
        cache: {
          ttl: 1e3
        }
      })
    );
  }
  models(inputOrCallback, optionsOrCallback, callback) {
    return handle(
      {
        inputOrOptionsOrCallback: inputOrCallback,
        optionsOrCallback,
        callback
      },
      async ({ options }) => {
        const { results } = await this.#fetcher({
          ...options,
          method: "GET",
          url: "/v1/models",
          id: generateCacheKey(CacheDiscriminator.MODELS)
        });
        return results;
      }
    );
  }
  model(input, optionsOrCallback, callback) {
    return handle(
      {
        optionsOrCallback,
        callback
      },
      async ({ options }) => {
        const { results } = await this.#fetcher({
          ...options,
          method: "GET",
          url: `/v1/models/${encodeURIComponent(input.id)}`,
          id: generateCacheKey(CacheDiscriminator.MODEL, input.id)
        });
        return results;
      }
    );
  }
  tunes(inputOrCallback, optionsOrCallback, callback) {
    return handleGenerator(
      {
        inputOrOptionsOrCallback: inputOrCallback,
        optionsOrCallback,
        callback
      },
      ({ input, options }) => {
        const params = new URLSearchParams();
        if (input?.filters?.search)
          params.set("search", input.filters.search);
        if (input?.filters?.status)
          params.set("status", input.filters.status);
        return paginator(
          (paginatorParams) => this.#fetcher({
            ...options,
            method: "GET",
            url: `/v1/tunes?${paginatorParams.toString()}`,
            cache: false
          }),
          {
            offset: input?.filters?.offset ?? void 0,
            count: input?.filters?.count ?? void 0,
            params
          }
        );
      }
    );
  }
  tune(input, optionsOrCallback, callback) {
    return handle({ optionsOrCallback, callback }, async ({ options }) => {
      let apiOutput;
      const isTuneInput = isTypeOf(input, "id" in input);
      if (isTuneInput) {
        const cacheKey = generateCacheKey(CacheDiscriminator.TUNE, input.id);
        const opts = options;
        if (opts?.delete) {
          await this.#fetcher({
            ...options,
            method: "DELETE",
            url: `/v1/tunes/${encodeURIComponent(input.id)}`,
            cache: {
              update: {
                [cacheKey]: "delete",
                [generateCacheKey(CacheDiscriminator.MODEL, input.id)]: "delete",
                [generateCacheKey(CacheDiscriminator.MODELS)]: "delete"
              }
            }
          });
          return;
        } else {
          apiOutput = await this.#fetcher({
            ...options,
            method: "GET",
            url: `/v1/tunes/${encodeURIComponent(input.id)}`,
            id: cacheKey
          });
        }
      } else {
        apiOutput = await this.#fetcher({
          ...options,
          method: "POST",
          url: `/v1/tunes`,
          data: input,
          cache: {
            update: {
              [generateCacheKey(CacheDiscriminator.MODELS)]: "delete"
            }
          }
        });
      }
      const { status } = apiOutput.results;
      switch (status) {
        case "COMPLETED":
          return {
            ...apiOutput.results,
            status,
            downloadAsset: async (type) => this.#fetcher({
              ...options,
              responseType: "stream",
              method: "GET",
              url: `/v1/tunes/${encodeURIComponent(
                apiOutput.results.id
              )}/content/${type}`,
              cache: false
            })
          };
        default:
          return { ...apiOutput.results, status };
      }
    });
  }
  tuneMethods(inputOrCallback, optionsOrCallback, callback) {
    return handle(
      {
        optionsOrCallback,
        callback
      },
      async ({ options }) => {
        const { results } = await this.#fetcher({
          ...options,
          method: "GET",
          url: `/v1/tune_methods`
        });
        return results;
      }
    );
  }
  promptTemplate(input, optionsOrCallback, callback) {
    return handle({ optionsOrCallback, callback }, async ({ options }) => {
      const isCreateInput = isTypeOf(
        input,
        !("id" in input)
      );
      if (isCreateInput) {
        const { results: result2 } = await this.#fetcher(
          {
            ...options,
            method: "POST",
            url: `/v1/prompt_templates`,
            data: input
          },
          PromptTemplateOutputSchema
        );
        return result2;
      }
      const endpoint = `/v1/prompt_templates/${encodeURIComponent(input.id)}`;
      const cacheKey = generateCacheKey(
        CacheDiscriminator.PROMPT_TEMPLATE,
        input.id
      );
      const opts = options;
      if (opts?.delete) {
        await this.#fetcher({
          ...options,
          method: "DELETE",
          url: endpoint,
          cache: {
            update: {
              [cacheKey]: "delete"
            }
          }
        });
        return;
      }
      const { id: _, ...body } = input;
      if (isTypeOf(body, !isEmptyObject(body))) {
        const { results: result2 } = await this.#fetcher(
          {
            ...options,
            method: "PUT",
            url: endpoint,
            data: body,
            cache: {
              update: {
                [cacheKey]: "delete"
              }
            }
          },
          PromptTemplateOutputSchema
        );
        return result2;
      }
      const { results: result } = await this.#fetcher(
        {
          ...options,
          method: "GET",
          url: endpoint,
          id: cacheKey
        },
        PromptTemplateOutputSchema
      );
      return result;
    });
  }
  promptTemplates(inputOrCallback, optionsOrCallback, callback) {
    return handleGenerator(
      {
        inputOrOptionsOrCallback: inputOrCallback,
        optionsOrCallback,
        callback
      },
      ({ input, options }) => paginator(
        async (paginatorParams) => this.#fetcher(
          {
            ...options,
            method: "GET",
            url: `/v1/prompt_templates?${paginatorParams.toString()}`,
            cache: false
          },
          PromptTemplatesOutputSchema
        ),
        {
          offset: input?.offset ?? void 0,
          count: input?.count ?? void 0
        }
      )
    );
  }
  promptTemplateExecute(input, optionsOrCallback, callback) {
    return handle({ optionsOrCallback, callback }, async ({ options }) => {
      const { results } = await this.#fetcher({
        ...options,
        method: "POST",
        url: "/v1/prompt_templates/output",
        data: input
      });
      return results;
    });
  }
  history(inputOrCallback, optionsOrCallback, callback) {
    return handleGenerator(
      {
        inputOrOptionsOrCallback: inputOrCallback,
        optionsOrCallback,
        callback
      },
      ({ input, options }) => {
        const params = new URLSearchParams();
        if (input?.status)
          params.set("status", input.status);
        if (input?.origin)
          params.set("origin", input.origin);
        return paginator(
          (paginatorParams) => this.#fetcher(
            {
              ...options,
              method: "GET",
              url: `/v1/requests?${paginatorParams.toString()}`,
              cache: false
            },
            HistoryOutputSchema
          ),
          {
            offset: input?.offset ?? void 0,
            count: input?.count ?? void 0,
            params
          }
        );
      }
    );
  }
  files(inputOrCallback, optionsOrCallback, callback) {
    return handleGenerator(
      {
        inputOrOptionsOrCallback: inputOrCallback,
        optionsOrCallback,
        callback
      },
      ({ input, options }) => paginator(
        async (paginatorParams) => this.#fetcher(
          {
            ...options,
            method: "GET",
            url: `/v1/files?${paginatorParams.toString()}`,
            cache: false
          },
          FilesOutputSchema
        ),
        {
          offset: input?.offset ?? void 0,
          count: input?.count ?? void 0
        }
      )
    );
  }
  file(input, optionsOrCallback, callback) {
    return handle({ optionsOrCallback, callback }, async ({ options }) => {
      const transformOutput = (apiOutput) => ({
        ...apiOutput,
        download: () => this.#fetcher({
          ...options,
          responseType: "stream",
          method: "GET",
          url: `/v1/files/${encodeURIComponent(apiOutput.id)}/content`,
          cache: false
        })
      });
      const isCreateInput = isTypeOf(input, !("id" in input));
      if (isCreateInput) {
        const { purpose, filename, file } = input;
        const formData = new import_form_data.default();
        formData.append("purpose", purpose);
        formData.append("file", file, { filename });
        const { results: result2 } = await this.#fetcher(
          {
            ...options,
            method: "POST",
            url: `/v1/files`,
            data: formData
          },
          FileOutputSchema
        );
        return transformOutput(result2);
      }
      const endpoint = `/v1/files/${encodeURIComponent(input.id)}`;
      const cacheKey = generateCacheKey(CacheDiscriminator.FILE, input.id);
      const opts = options;
      if (opts?.delete) {
        await this.#fetcher({
          ...options,
          method: "DELETE",
          url: endpoint,
          cache: {
            update: {
              [cacheKey]: "delete"
            }
          }
        });
        return;
      }
      const { results: result } = await this.#fetcher(
        {
          ...options,
          method: "GET",
          url: endpoint,
          id: cacheKey
        },
        FileOutputSchema
      );
      return transformOutput(result);
    });
  }
  chat(input, optionsOrCallback, callback) {
    const { callback: cb, options } = parseFunctionOverloads(
      void 0,
      optionsOrCallback,
      callback
    );
    if (options?.stream) {
      const stream = new import_node_stream.Transform({
        autoDestroy: true,
        objectMode: true,
        transform(chunk, encoding, callback2) {
          const { results, ...rest } = chunk;
          callback2(null, {
            ...rest,
            result: results[0]
          });
        }
      });
      this.#fetcher({
        ...options,
        method: "POST",
        url: "/v0/generate/chat",
        data: {
          ...input,
          parameters: {
            ...input.parameters,
            stream: true
          }
        },
        stream: true
      }).on("error", (err) => stream.emit("error", errorTransformer(err))).pipe(stream);
      if (cb) {
        callbackifyStream(stream)(cb);
        return;
      } else {
        return stream;
      }
    } else {
      const promise = (async () => {
        const { results, ...rest } = await this.#fetcher(
          {
            ...options,
            method: "POST",
            url: "/v0/generate/chat",
            data: input,
            stream: false
          },
          ChatOutputSchema
        );
        if (results.length !== 1) {
          throw new InternalError("Unexpected number of results");
        }
        return { ...rest, result: results[0] };
      })();
      if (cb) {
        callbackifyPromise(promise)(cb);
        return;
      } else {
        return promise;
      }
    }
  }
};

// src/langchain/llm.ts
var GenAIModel = class extends import_base.BaseLLM {
  #client;
  modelId;
  promptId;
  isStreaming;
  timeout;
  parameters;
  constructor({
    modelId,
    promptId,
    stream = false,
    parameters,
    timeout,
    configuration,
    ...baseParams
  }) {
    super(baseParams ?? {});
    this.modelId = modelId;
    this.promptId = promptId;
    this.timeout = timeout;
    this.isStreaming = Boolean(stream);
    this.parameters = parameters || {};
    this.#client = new Client(configuration);
  }
  #createPayload(prompts, options) {
    const stopSequences = concatUnique(this.parameters.stop, options.stop);
    return prompts.map((input) => ({
      ...!isNullish(this.promptId) ? {
        prompt_id: this.promptId
      } : !isNullish(this.modelId) ? {
        model_id: this.modelId
      } : {},
      input,
      parameters: {
        ...this.parameters,
        stop_sequences: isNotEmptyArray(stopSequences) ? stopSequences : void 0
      }
    }));
  }
  async #execute(prompts, options) {
    return await Promise.all(
      this.#client.generate(this.#createPayload(prompts, options), {
        signal: options.signal,
        timeout: this.timeout,
        stream: false
      })
    );
  }
  async _generate(prompts, options, runManager) {
    const response = [];
    if (this.isStreaming) {
      const { output } = await asyncGeneratorToArray(
        this._streamResponseChunks(prompts[0], options, runManager)
      );
      response.push(output);
    } else {
      const outputs = await this.#execute(prompts, options);
      response.push(...outputs);
    }
    const generations = response.map(
      ({ generated_text: text, ...generationInfo }) => [
        {
          text,
          generationInfo
        }
      ]
    );
    const llmOutput = await response.reduce(
      (acc, generation) => {
        acc.generated_token_count += generation.generated_token_count;
        acc.input_token_count += generation.input_token_count;
        return acc;
      },
      {
        generated_token_count: 0,
        input_token_count: 0
      }
    );
    return { generations, llmOutput };
  }
  async *_streamResponseChunks(_input, _options, _runManager) {
    const [payload] = this.#createPayload([_input], _options);
    const stream = this.#client.generate(payload, {
      signal: _options.signal,
      timeout: this.timeout,
      stream: true
    });
    const fullOutput = {
      generated_text: "",
      stop_reason: "NOT_FINISHED",
      input_token_count: 0,
      generated_token_count: 0
    };
    for await (const { generated_text, ...chunk } of stream) {
      const generation = new import_schema.GenerationChunk({
        text: generated_text,
        generationInfo: chunk
      });
      yield generation;
      void _runManager?.handleLLMNewToken(generated_text);
      fullOutput.generated_text += generation.text;
      if (chunk.stop_reason) {
        fullOutput.stop_reason = chunk.stop_reason;
      }
      fullOutput.input_token_count += chunk.input_token_count;
      fullOutput.generated_token_count += chunk.generated_token_count;
    }
    return fullOutput;
  }
  async getNumTokens(input) {
    const result = await this.#client.tokenize({
      ...!isNullish(this.modelId) && {
        model_id: this.modelId
      },
      input,
      parameters: {
        return_tokens: false
      }
    });
    return result.token_count ?? 0;
  }
  _modelType() {
    return this.modelId ?? "default";
  }
  _llmType() {
    return "GenAI";
  }
};

// src/langchain/llm-chat.ts
var import_base2 = require("langchain/chat_models/base");
var import_schema2 = require("langchain/schema");
var GenAIChatModel = class extends import_base2.BaseChatModel {
  #model;
  #rolesMapping;
  constructor(options) {
    super(options);
    this.#rolesMapping = options.rolesMapping;
    this.#model = new GenAIModel({
      ...options,
      parameters: {
        ...options.parameters,
        stop_sequences: concatUnique(
          options.parameters?.stop_sequences,
          Object.values(options.rolesMapping).map((role) => role.stopSequence)
        )
      },
      configuration: {
        ...options.configuration,
        retries: options.maxRetries ?? options.configuration?.retries
      }
    });
  }
  async _generate(messages, options, runManager) {
    const message = messages.map((msg) => {
      const type = this.#rolesMapping[msg._getType()];
      if (!type) {
        throw new InvalidInputError(
          `Unsupported message type "${msg._getType()}"`
        );
      }
      return `${type.stopSequence}${msg.text}`;
    }).join("\n").concat(this.#rolesMapping.system.stopSequence);
    const output = await this.#model._generate([message], options, runManager);
    return {
      generations: output.generations.map(([generation]) => ({
        message: new import_schema2.SystemMessage(generation.text),
        generationInfo: generation.generationInfo,
        text: generation.text
      })),
      llmOutput: output.llmOutput
    };
  }
  _combineLLMOutput(...llmOutputs) {
    return llmOutputs.reduce(
      (acc, gen) => {
        acc.tokenUsage.generated_token_count += gen.generated_token_count || 0;
        acc.tokenUsage.input_token_count += gen.input_token_count || 0;
        return acc;
      },
      {
        tokenUsage: {
          generated_token_count: 0,
          input_token_count: 0
        }
      }
    );
  }
  _llmType() {
    return "GenAIChat";
  }
  _modelType() {
    return this.#model._modelType();
  }
};

// src/langchain/prompt-template.ts
var import_prompts = require("langchain/prompts");
var GenAIPromptTemplate = class _GenAIPromptTemplate {
  static toLangChain(template) {
    const body = typeof template === "string" ? template : template.value;
    const fString = body.replace(
      _GenAIPromptTemplate.getTemplateMatcher("mustache"),
      "{$1}"
    );
    return import_prompts.PromptTemplate.fromTemplate(fString, {
      templateFormat: "f-string",
      validateTemplate: true
    });
  }
  static fromLangChain(template) {
    return template.template.replace(
      _GenAIPromptTemplate.getTemplateMatcher(template.templateFormat),
      "{{$1}}"
    );
  }
  static getTemplateMatcher(name) {
    switch (name) {
      case "mustache":
        return /\{\{([^}]+)\}\}/g;
      case "jinja2":
        return /\{\{\s*(.*?)\s*\}\}/g;
      case "fstring":
      case "f-string":
        return /\{([^}]+)\}/g;
      default: {
        throw new InvalidInputError(`Unknown template format "${name}".`);
      }
    }
  }
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  GenAIChatModel,
  GenAIModel,
  GenAIPromptTemplate
});
